---
title: "Applied Microeconometrics""
author: "Ji Hun Lee"
date: "April 13, 2020"
output: html_document
---

# Introduction

The goal of this study is to establish causal effect of *education on wages*. Note that this is fundamentally a different question from "Can we use  *education to forecast wage*" (predictive) or "How does *education vary with wage*" (descriptive).

Our data is collected via only observing the outcome - *wage* - in the actual choice scenario of *enrolling in higher education*. It would be great if we can construct counterfactual outcomes to answer the question: what would happen if *an individual had a different educational attainment*. Not being able to observe disparate outcomes *(educated vs uneducated)* at the same time for the same individual experimental unit is hard.

Assessing empirical data, Causal inference is the best done with rando mized control trials because they generate counterfactual data. However, most often we only have observational data. This causes identification problems because many different theoretical models and many diffferent causal interpretations can be consistent with the same data.


# Variables
This data is *the NLS sample of young men containing information on hourly wage in cents (wage) and years of schooling (educ) in 1976*. Since we want to estimate the causal effect of *education on wages*, our response variable is *wage* and our main variable of interest is *educ*. 

```{r}
library(haven)
library(tidyverse)

df <- read_dta('C:/Users/jihun/Downloads/applied_microeconometrics/CARD.DTA')
glimpse(df)

```

What would be the ideal experiment? It helps to think about what our ideal experiment would be.

# Identification Strategy


A good identification strategy is a clearly specified source of identifying variation in a causal variable, combined with particular econometric technique to exploit this information. Randomized trials ensure that outcomes in a control group capture the counterfactual in a treatment group, but RCT are rare in practice. 

Econometrics applies model to data and rigorously testing the assumptions of model to make a statement about causality. More specifically, econometrics attempts to address challenges of observational data: confounding effects (omitted variables), simultaneity, and correlation doesnt imply causation. In order to analyze observational studies, we can employ the following identification strategies.

1. Regression: control for observable differences between comparison groups
2. Instrumental Variable and Regression Discontinuity use exogenous source of variation
3. Difference in Differences: use pre-post comparisons of control and treatment groups to control for unobservable differences
4. Panel data techniques: use pre-post comparisons on the same unit of observvation to control for fixed unobservable differences

### Simultaneity, Self-Selection, Ethical Considerations, Measurement Error

Ideal experiment for non-randomly selected variables? Are they feasible? 

Are they ethical? 

Is it meaningful to estimate causal effect of variable (like gender)?

Can we suspect variable to be truthfully reported by subjects? What issues can arise if variable is measured with error. How does this affect OLS estimates?  Underreporting of variable like drinking habit can cause inconsistent OLS estimates. Coefficient will be biased if there is a correlation between measurement error and reported drinking. Measurement error always increases standard errors. Measurement error's internal variance amplifies the uncertainty of estimate.

Measurement error can cause attenuation bias, causing estimate to shrink toward zero.


# I. Ordinary Least Squares

Regression Technique makes inferences about unknown population slope coefficients - *in this case the effect of education on wage*. For causal inference, we need to ccontrol for observable differences between comparison groups.


## Control Variable

In order to reduce omitted variable bias, we need to include control variables into our regression model. We will control for *intelligence* of our subjects to control for systematic variation of wage with respect to subject's *intelligence*.
```{r}
library(jtools)
lmod1 <- lm(lwage~educ+KWW, data=df)
lmod1$call$formula

```

### Overcontrolling

We need to ensure whether our model has any Overcontrolling issues. In our case, there is *no* overcontrolling. For example, we don't need to include productivity as a variable when we already have education, IQ. 

## Experimental Restriction

As discussed earlier, we need to keep in mind of survey design restriction because it can affect the assumptions needed for oLS to have valid results. For instance, *IQ needs to be measured prior to education because education can increase intelligence and dilute the effect of education. We want to measure KWW before high school such that we avoi having a measure of ability that is affected by education. For example, one could imagine that ability is increasing more for those who are in high school or college instead of having dropped out.* We also need to be mindful of whether our variable of interest is something that can be chosen (e.g. gender is something not chosen). *In our case our variable is not gender, then interpretation of coefficient is, difference between average of male response and female response. *

```{r}
ggplot(aes(x=gender, y=gpa, fill=gender)) +
  geom_box() +
  theme_minimal() +
  labs(title='Boxplot',
       x = 'gender',
       y = 'gpa')
  
```


## Data Preprocessing

We need to choose data preprocessing before running our econometric methods. *In our example, we need to decide whether a categorical or numeric variable is better suitable. Depending on how we encode our educ variable as numeric or categorical, interpretation becomes very different. For instance, is education better in terms of years in education or degree earned? College degree will be educ >= 16, HS will be >= 12 and <16. The question will ask whether the wage only responds to the degree i.e. whether wage jumps at the moment an individual obtains a degree and do not change everywhere else, or the wage increases linearly with years of education.*

 Log Transformation: can be used to normalize the response variable, but changes *wage* to percentages, not levels. The choice of log-transformation depends on whether we will define the response in terms of percentage or original unit.
   - benefit 1: often fit CLM assumption better -> normalize error and fix heteroskedasticity
   - benefit 2: can be a better, more plausible functional form in linear regression
   - benefit 3: easier to interpret - approximate percentage changes
   - WARNING: if you log,log(10,000) - log(1) = 9.2 whereas log(1,500,000) - log(80,000) = 0.63. This implies variation in the data will be due to whether someone is - for example in the job market, rather than effect on wages.
   
*Our model will log-transform the wage variable.*

```{r}
library(tidymodels)
df <-
  recipe(lwage ~ educ + KWW + wage + age + exper, data=df) %>%
    step_mutate(COL = ifelse(educ >= 16, 1, 0), # college degree is people who have more than 16 years of educatioanl years
         HS = ifelse(educ >= 12 & educ < 16, 1, 0)) %>%  # high school degree is people who have at least 12 years
    prep(data=df) %>%
    juice()
glimpse(df)
lmod2 <- lm(lwage ~ COL + HS + KWW, data=df)
```

## Interpretation of Coefficients
```{r}
print(summ(lmod1))
print(summ(lmod2))

```

Interpretation of coefficient needs to be in the presence of control. We need to state, 'holding control's value constant/fixed, every unit of Y is increased/decreased by every unit change in X on average'. If  Y is logged, then change in X by 1 Y changes by 100\*exp(beta\*X) percentage; if coefficiet is 0.029 then Y changes by 2.9% 

As shown in the table above, the coefficient for *educ is 0.02.*
*As shown in the second table above, the coefficient for HS and COL are 0.13 and 0.2, respectively.*
We can interpret the number *0.02* as a *'person would have received 2.1% higher wage on average if the person had one year longer education and had the same KWW.'* The estimated coefficient measures the relationship between *lwage* and the unique variation in *educ* - partialling out.
    
Is it statistically different from zero, vary signiifor in other words, can we reject the null hypothesis that beta = 0? does it vary significantly with response? if it is not, we dont have to allow for the variable. *Yes, it is statistically significant.*

*F-test rejects the null hypothesis that the coefficients on the two types of education return are equal.*

### Functional Form

We need to check the functional relationship between our numeric variable of interest and response. *In our case, the linear relationship seems to hold.*
```{r}

df1 <- 
  df %>%
  mutate(KWW_bins =  predict(
    discretize(df$KWW, na.rm=T, infs=F, cuts=3),
    df$KWW))

df1 %>%
  ggplot(aes(x=educ, y=lwage)) +
    geom_count() +
    geom_smooth(method='loess') +
    labs(title='Scatterplot',
         subtitle='Explore Functional Relationship',
         x='Education in Years',
         y='Log Wage') +
    theme_minimal() +
    facet_wrap(~KWW_bins, ncol=2)

```

### Interaction Effects
  
We need to check whether there is any interaction effect on the variable of interest with any other control variables. *In our case, we want to check for interaction between the number of years in schooling and degrees. For instance, we want to know if additional year of schooling for high school degree is statistically varying with lwage. We can see that having additional year of schooling after HS degree is not statistically significant on wage level, but it is for college degree. We test whether the coefficient for interaction is zero to check whether the return to education differs by years of schooling. The p-value for educ x college is quite lower than the usual threshold of 0.05 and we can think of this as an evidence that the return to education differs according to the years of schooling after college degree. The return to education is about 42 units higher compared to the one with zero schooling*

```{r}
lmod3 <- lm(wage ~ educ + KWW + educ*HS + educ*COL, data=df)
summ(lmod3)
```



### Multicollinearity

We should always check for multicolinearity in the data because in the presence of collinearity, there is little unique variation in each X and it is hard to disentangle effect of one X from other correlated X. It makes inference more difficult because it causes bias and increases standard error in our coefficients. *There is no pronounced collinearity in our data.*

```{r}
library(GGally)
print(ggcorr(df, method = c("everything", "pearson")))
print(ggpairs(df, progress=F))
```

### Omitted Variables

Omitted Variables create bias in coefficients. 
  - When correlation between X1 and X2 is positive and omitted variable has positive slope, then it inflates slope.
  - When correlation between X1 and X2 is negative and omitted variable has negative slope, then it inflates slope. 
  - When correlation between X1 and X2 is positive and omitted variable has negative slope, then it deflates slope.
  - When correlation between X1 and X2 is negative and omitted variable has positive slope, then it deflates slope.
We can see what the sign of bias was by how the coefficient changed. If it increased, then bias was negative. If it decreased, then bias was positive. If coefficient on control is positive and bias is positive, then correlation between control and variable must be positive. If new control is not significant and other coefficients do not change.
  
Omitted variable is a problem because it splits the correlation between observed variable X1 and omitted variable X2. However, when two independent variables are uncorrelated, there is no omitted variable bias.

One way to check omitted variable bias is to see how coefficients vary based on including the omitted variable. 
```{r}
lmod4 <- lm(lwage ~ HS + COL + KWW + age, data=df)
summ(lmod4)
```

We check the sensitivity of coefficients and statistical significiance. *There is a little change in the coefficient values after inclusion of age control variable. Since the coefficients have increased, we can claim that the omitte variable bias was negative and deflated the true value of college's wage premium. Older people tend not to have college degree so there is a negative correlation between college degree and age. It is important to note that all the existing variables are still statistically significant after controlling for age variable.*

The only way to remove omitted variable bias in the variable of interest is to add additional determinants of Y and control them.

## OLS Assumption Validation

Validate assumtions
  1. linear in parameters
  2. random sampling
  3. no perfect colliearity among variables
  4, zero conditional mean needs to hold
  5. homoscedasticity: error variance is not the same across all values of the independent variables
  
### Zero Conditional Mean

Violation of this assumption causes a biased estimate.

We prove coefficient is unbiased (4th assummption) by showing that for every combination of variable of interest and control (zero conditional mean assumption), mean value is around 0 by creating group_by table and see if there is any systematic variation. Also, we use fitted value versus residual plot and see if residuals are around 0 horizontal line. If there is a pattern, then there is an omitted variable bias which is creating systematic variation in response not accounted for by control or variable.

Another way is to check the diagnostic plots, in particular residual vs fitted value plot to see if the mean of residuals per fitted value is zero.

```{r}
plot(lmod4)
df %>%
  mutate(residuals = residuals(lmod))
  groupby(educ, KWW) %>%
  summarise(conditional.mean = mean(residuals))


```



## Precision of Estimates
   
Precision of estimates is dependent on:
  - small collinearity
  - large sample size 
  - big variation within the variable of interest

### Homoscedasticity Check

Heteroskedasticity does not affect unbiasedness of OLS estimates but creates bias of variance estimation - standard errors. Standard error is a measure of how much beta will vary across different samples of the same size from the same population. Heteroskedasiticty can inflate or deflate standard errors because of presence of influential data points. when you have observations far from the mean (outliers), they are more informative of the slope coefficient and has greater weight in influencing its value. However, outliers are usually subject to a lot of noise, and random chance due to noise plays a big role in the value of coefficient in any given sample. Slope estimate will vary a lot across samples due to different values of the error term for the most informative observations, which are more likely to happen in the presence of heteroskedasticity.

Check homoscedasticity assumption by White tests, and Breuch-Pagan test. Look at the standard errors and residual plot (see if there are large dispersions).

Ignoring heteroskedasticity will imply we put too much faith iin beta from a given sample i.e. underestimate the variance. Because it depends a lot onstructure of data, it is not clear how heteroskedasticity affects variance.

```{r}
library(lmtest)
library(bstats)

# Breuch Pagan Test
bptest(gpa ~ gender + SAT, data = df) # run regression on squared residuals, get its R-squared and compute F-statistic

# White Test
white.test(lmod)# hypothesis testing on the statistic: sum of squared deviations from the mean * residuals divided by sum suqare total

```

Ways to deal with heteroskedasticity:
  - transform the model (e.g. log-transform)
  - use robust standard errors
  - specify the form of error variance and use WLS

Run heteroskedasticity-robust linear regression.

```{r}
# generate robust stanard errors
coeftest(lmod, vcov = vcovHC(lmod))

# robust linear regression
library(robustbase)
lmod6 <- lmrob(gpa ~ sex + sat, data=df)
summary(lmod6)

```

## Evaluating Model

R-squared: fraction of sample variation in Y that is explained by all the explanatory ariables. It always increases when more variables are added to regression. Low R squared implies it is difficult to predict individual outcomes.

Adjusted R-squared: may increase or decrease with addition of another regressor.

We can use these metricsc to determine whether some functional form of variable or interaction term is better or not based on whether adjusted R squared changes

```{r}

# RMSE

# MAE

# MAPE

```
   

# Instrumental Variable Regression

## Intro
They are helpful when randomized experient is not avaiable or when all unobserved confounding variables are present
Used when my model has endogenous X, i.e. conditional mean of error is not zero. This renders OLS estimator inconsistent and cannot be abscribed causal interpretation. Conditional mean is not zero when 1) omitted variable bias 2) selection bias 3) simultaneity of Y and X 4) error in measurement can cause nonzero
- instruments must satisfy - Relevance (correlated with X and Exogeneity (uncorrelated with error U)
- when instruments are valid (relevant and exogenous), estimators become consistent
- We can test relevance but not exogeneity (need to use common sense and economic theory)

21. 2SLS: 2 stage least squares
- 1st stage:  regress instrument Z on X  and obtain estimated X
- 2nd stage: regress estimated X on Y
- It is inconsistent when covariance between error U and instrment Z is 0
- estimator is asymptotically normally distributed
- standard errors are smaller when correlation between instrument Z and X is stronger
- However, IV standard error will be always larger than OLS error

IV solves endogeneity (omitted variable bias) problem if IV is a valid instrument because IV uses the exogenous part of the variation of X that is uncorrelated with error U to estimate the causal effect of X on Y. 

## Identification Strategy of Instrument and Practical Tips


## Reduced Form (First Stage)
State the formula. 
Obtain the F-test. Check if it is larger than 10.

Coefficient values and standard errors

Test for the significance of instruments


## Assumptions
State which assumptions need to be fulfilled for IV to be valid instrument for variable. Two conditions are 1) exogeneity and 2) relevance. One condition implicit in the exogeneity assumption is exclusion: IV should not directly affect Y. Exogeneity implies IV should not affect Y through an omitted variable. 

One way to argue exogeneity condition holds true is if it is well randomized.
### Exogeneity

Test IV's exogeneity. Is it testable? 

What possible violations are there?

Test by overidentification, tets hypothesis that all instruments are exogenous assuming that at leat one instrument is exogenous, but we cannot test the assumption that at leat one instrument is exogenous.

If IV is not exogenous, then IV estimator will be biased.

## Weak Instrument Problem

Weak instrument will lead to a very high standard error of IV estimator. If we dont have exogeneity, then IV estimator will be biased and will have high SE.

If instrument is weak and is not exogenous, then the IV estimator can be very misleading. In this case, OLS is biased but we know the direction and sign of bias, then we may use the OLS estimato as the bound of the true value. For example, if we know the OLS estimator is biased upward, then we can think of OLS as the upper bound of the true value. 

Weak instrument problem compromises internal validity.

IV estimator converges to OLS estimator. 

## Problem of Too Many Instruments
Having many instruments leads to having a large biass and havving small standard error.

## Generalizability

Internal Validity and External Validity
There is a tradeoff between the two
Can we extrapolate the results to other units (e.g. location, period)?
Sample large = external validity increases

### Heterogeneous Treatment and LATE estimate
LATE estimate = can make it difficult to generalize for different subpopulations - compliers, etc




# Regression Discotinuity
## What is Regression Discontinuity?
Run a regression in a situation where you have a discontinuity. How very similar people behave when they get dissimilar treatemnts. It exploits precise knowledge of the rules determining treatment. Works on quasi-experiment. Main identifying assumption: in a sufficiently near neighborhood around the discontinuity treatment is as good as randomly assigned. 

### examples:
What kind of problems does it deal with? 
1. effect of scholarship on students
2. effect of class size on student performance
3. effect of unions
4. effect of air pollution

## Problem Setup
What is the outcome of interest?

What is the running variable, X?

What is the treatment variable D and how is it determined by X? A discontinuous function of X from a smooth and flexible function that controls the counterfactual. RD captures a causal effect by distinguish the treatment D from variable X.

Interaction variable

# What are the assumptions? Are they more credifying assumptions than multiple linear regression?
MLR.4 may not hold because of omitted variable bias. Then, RD is more credible.

## Type of RDD
Types of RDD: 
1. Sharp:  treatment is a deterministic, discontinuous function of a covariate X, a selection-on-observables story; for example, when treatment variable is binary. This design is based on selection on observables assumptions: Once we know X, we know D. At a certain vlaue of X, the probability of treatment D jumps discontinuously. D is correlated with X. We estimate causal effect by distinguishing D from f.
- key identifying assumption: conditional mean of Y on x is continuous on X. But this assumption is not testable.
- two ways of approximation: 1) nonparametric kernel method 2) kth order polynomial
2. Fuzzy: when treatment variable is numeric or has many categories; exploits discontinuities in the probability of treatment. An instrument IV type of setup

Is this a sharp design or fuzzy?



## RDD Design
Restrict sample size
Repercussion of smaller sample size - what is the tradeoff?


Comment on the internal and external validities of RDD

## Fitting Model
Run local linear regression on various bandwidths

Visualize them for different bandwidths

## Regression Summary

Regression Result - coefficient interpretation

## Visualization
1. Show scatterplot with vertical line 
2. Density plots before and after trimming 15% and 85%
3. Different bandwidths and regression coefficient
4. 

## Assumptions
Evaluation assumptions


## Instrumental Variable
How does IV work in RDD?
Instrumental Variable - what are the implications on internal and external validities of RDD?




